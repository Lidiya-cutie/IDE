# Библиотека BeautifulSoup

**BeautifulSoup** не является частью стандартной библиотеки, поэтому для начала её нужно установить. Например, в *Jupyter Notebook* это делается с помощью такой команды:

```python
# Устанавливаем библиотеку BeautifulSoup
!pip install beautifulsoup4
```
```python
# Импортируем библиотеку BeautifulSoup
from bs4 import BeautifulSoup
```
Теперь мы можем извлекать данные из любой веб-страницы.

Cоздадим объект *BeautifulSoup* с именем *page*, указывая в качестве параметра *html.parser*.

Для примера получим информацию o *title* (с англ. *заголовок*) — это строка, которая отображается на вкладке браузера:
```python
# Импортируем библиотеку requests
import requests
# Импортируем библиотеку BeautifulSoup
from bs4 import BeautifulSoup
# Определяем адрес страницы
url = 'https://nplus1.ru/news/2021/10/11/econobel2021'
# Выполняем GET-запрос, содержимое ответа присваивается переменной response
response = requests.get(url)
# Создаём объект BeautifulSoup, указывая html-парсер
page = BeautifulSoup(response.text, 'html.parser')
# Получаем тег title, отображающийся на вкладке браузера
print(page.title)
# Выводим текст из полученного тега, который содержится в атрибуте text
print(page.title.text)
```
# ИЗВЛЕКАЕМ ЗАГОЛОВОК И ВРЕМЯ НАПИСАНИЯ СТАТЬИ

Выполним поставленную ранее задачу: получить информацию о странице и извлечь заголовок статьи, опубликованной на этой [странице](https://nplus1.ru/news/2021/10/11/econobel2021), дату публикации, а также текст статьи.

Предположим, что мы знаем, что в *HTML*-коде рассматриваемой нами страницы заголовок статьи заключён в тег ```<h1> ```… ```</h1>``` (заголовок первого уровня).

Тогда мы можем получить его текст с помощью метода *find()* (с англ. найти) объекта *BeautifulSoup*, передав ему название интересующего нас тега:
```python
# Импортируем библиотеку requests
import requests #
# Импортируем библиотеку BeautifulSoup
from bs4 import BeautifulSoup 
# Определяем адрес страницы
url = 'https://nplus1.ru/news/2021/10/11/econobel2021'
# Выполняем GET-запрос, содержимое ответа присваивается переменной response
response = requests.get(url)
# Создаём объект BeautifulSoup, указывая html-парсер
page = BeautifulSoup(response.text,'html.parser')
# Применяем метод find() к объекту и выводим результат на экран
print(page.find('h1').text)
```

Напишем функцию wiki_header, которая по адресу страницы возвращает заголовок первого уровня для статей на Wikipedia.

Функция wiki_header принимает один аргумент - url.

```python
import requests
from bs4 import BeautifulSoup

def wiki_header(url):
    page = BeautifulSoup(requests.get(url).text, 'html.parser')
    header = page.find('h1').text
    return header

url = 'https://en.wikipedia.org/wiki/Operating_system'

print(wiki_header(url = 'https://en.wikipedia.org/wiki/Operating_system'))

# Operating system - результат выполнения кода
```

# НЕУНИКАЛЬНЫЕ ТЕГИ: ИЗВЛЕКАЕМ ТЕКСТ И ДАТУ ПУБЛИКАЦИИ СТАТЬИ

Теперь получим сам текст статьи.  Применим, как и ранее, инструмент разработчика.

Видим, что искомый текст заключён в тег  ```<div>``` … ```</div>``` . Попробуем извлечь его уже известным нам способом — с помощью метода *find()* — и выведем его на экран.
```python
# Выводим содержимое атрибута text тега div
print(page.find('div').text)
```

Но увидим не то, что ожидали — кучу текста, не имеющего отношения к тому, что мы искали...

Дело в том, что теги `<div>` … `</div>` очень распространённые и на странице их очень много. Метод *find()* нашёл первый из них, но это не то, что нам надо.

Посмотрим на нашу страницу, используя инструмент разработчика, ещё раз. Можем заметить, что у искомого текста есть свой класс — *n1_material text-18* :

Передадим название класса в метод *find()* с помощью аргумента *class_* и получим текст статьи:
```python
# Выводим содержимое атрибута text тега div класса n1_material text-18
print(page.find('div', class_ = 'n1_material text-18').text)
```
В данном случае происходит поиск точного строкового значения class атрибута, т. е. выполнение строк кода:

* print(page.find('div', class_='n1_material').text)
* print(page.find('div', class_='n1_material text-18').text)

даст одинаковый результат.

Аналогично получим информации о теге, который содержит дату написания статьи, отображаемую в левом верхнем углу страницы.

Итак, нам нужен тег `<a>` … `</a>` с классом *"relative before:block before:w-px before:bg-current before:h-4 before:absolute before:left-0 group pl-2 flex inline-flex items-center"*. Для поиска достаточно указать в качестве класса ***"relative"***, отбросив дополнительные настройки.

Теперь получим данные из него с помощью уже известного метода *find()*, передав название нужного тега:
```python
# Выводим на экран содержимое атрибута text тега a с классом "relative"
print(page.find('a', class_ = "relative").text)

# 11.10.21 - результат выполнения кода
```
О поиске по классу можно узнать подробнее в [Beautiful Soup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#searching-by-css-class).

# СБОР НЕСКОЛЬКИХ ЭЛЕМЕНТОВ: СОБИРАЕМ ВСЕ ССЫЛКИ НА СТРАНИЦЕ

Рассмотрим ещё один сценарий: вы хотите собрать сразу несколько элементов со страницы. Например, представьте, что вы хотите получить названия всех языков программирования, упомянутых на ***[странице в Wikipedia](https://en.wikipedia.org/wiki/List_of_programming_languages)*** в статье про языки программирования.

Можно заметить, что все названия языков программирования на этой странице связаны ссылками c соответствующими статьями о них. Таким образом, нам необходимо собрать все ссылки на странице. Для ссылок в HTML предусмотрен тег `<a>` … `</a>`. Попробуем использовать *find()*:
```python
# Задаём адрес ресурса
url = 'https://en.wikipedia.org/wiki/List_of_programming_languages'
# Делаем GET-запрос к ресурсу
response = requests.get(url)
# Создаём объект BeautifulSoup
page = BeautifulSoup(response.text, 'html.parser')
# Ищем ссылку по тегу <a> и выводим её на экран
print(page.find('a'))

# <a class="mw-jump-link" href="#bodyContent">Jump to content</a> - результат выполнения кода
```
Мы получили только одну ссылку, хотя на странице их явно больше.

Это происходит, потому что метод *find()* возвращает только первый подходящий элемент. Если требуется получить больше элементов, необходимо воспользоваться методом *find_all()* (с англ. *найти все*):

```python
links = page.find_all('a') # Ищем все ссылки на странице и сохраняем в переменной links в виде списка
print(len(links)) # Выводим количество найденных ссылок

# 953 - результат выполнения данного кода
```
Итак, на момент создания этих учебных материалов на странице содержалось 953 ссылки. Посмотрим на некоторые из них:
```python
# Выводим ссылки с 500 по 509 включительно
print([link.text for link in links[500:510]]) 

# ['MAD', 'MAD/I', 'Magik', 'Magma', 'Máni', 'Maple', 'MAPPER', 'MARK-IV', 'Mary', 'MATLAB'] - результат выполнения данного кода
```

Не все ссылки соответствуют названиям языков программирования — страница содержит также «служебные» ссылки, такие, например, как *Jump to navigation* (с англ. *Перейти к навигации*) или *Alphabetical* (с англ. *По алфавиту*):
```python
# Выводим ссылки с 1 по 9 включительно
print([link.text for link in links[0:10]]) 

# ['Jump to content', '\n\n\n\n\n\n', '\nSearch\n', 'Create account', 'Log in', ' Create account', ' Log in', 'learn more', 'Contributions', 'Talk'] - результат выполнения данного кода
```

Мы рассмотрели её базовые возможности, но их полный список гораздо шире. С ним можно ознакомиться в [официальной документации](https://www.crummy.com/software/BeautifulSoup/bs4/doc/).